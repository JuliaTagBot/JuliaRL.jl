var documenterSearchIndex = {"docs":
[{"location":"index.html#JuliaRL.jl-Documentation-WIP-1","page":"Home","title":"JuliaRL.jl Documentation - WIP","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"Current Working example: misc/testlinearqagentmountaincar.jl","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"More examples and explanations to come","category":"page"},{"location":"docs/environments.html#","page":"Environments","title":"Environments","text":"CurrentModule = JuliaRL","category":"page"},{"location":"docs/environments.html#","page":"Environments","title":"Environments","text":"AbstractEnvironment\nstart!(env::AbstractEnvironment; rng::AbstractRNG, kwargs...)\nstart(env::AbstractEnvironment; rng::AbstractRNG, kwargs...)\nstep!(env::AbstractEnvironment, action; rng::AbstractRNG, kwargs...)\nstep(env::AbstractEnvironment, action; rng::AbstractRNG, kwargs...)","category":"page"},{"location":"docs/environments.html#JuliaRL.AbstractEnvironment","page":"Environments","title":"JuliaRL.AbstractEnvironment","text":"Represents an abstract environment for reinforcement learning agents. Has several functions that need to be implemented to work\n\n\n\n\n\n","category":"type"},{"location":"docs/environments.html#JuliaRL.start!-Tuple{AbstractEnvironment}","page":"Environments","title":"JuliaRL.start!","text":"start!(env::AbstractEnvironment; rng::AbstractRNG, kwargs...)\n\nFunction to start the passed environment env.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.start-Tuple{AbstractEnvironment}","page":"Environments","title":"JuliaRL.start","text":"start(env::AbstractEnvironment; rng::AbstractRNG, kwargs...)\n\nFunction to retrieve a started environment struct without effecting the current environment struct. Returns an environment of the same type.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.step!-Tuple{AbstractEnvironment,Any}","page":"Environments","title":"JuliaRL.step!","text":"step!(env::AbstractEnvironment, action; rng::AbstractRNG, kwargs...)\n\nUpdate the state of the passed environment env based on the underlying dynamics and the action.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.step-Tuple{AbstractEnvironment,Any}","page":"Environments","title":"JuliaRL.step","text":"step!(env::AbstractEnvironment, action; rng::AbstractRNG, kwargs...)\n\nUpdate the state of the environment based on the underlying dynamics and the action.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#","page":"Environments","title":"Environments","text":"The above functions take advantage of the following interface.","category":"page"},{"location":"docs/environments.html#","page":"Environments","title":"Environments","text":"JuliaRL.reset!(env::AbstractEnvironment; rng::AbstractRNG, kwargs...)\nJuliaRL.environment_step!(env::AbstractEnvironment, action; rng::AbstractRNG, kwargs...)\nJuliaRL.get_reward(env::AbstractEnvironment)\nJuliaRL.is_terminal(env::AbstractEnvironment)\nJuliaRL.get_state(env::AbstractEnvironment)\nJuliaRL.get_actions(env::AbstractEnvironment)","category":"page"},{"location":"docs/environments.html#JuliaRL.reset!-Tuple{AbstractEnvironment}","page":"Environments","title":"JuliaRL.reset!","text":"reset!(env::AbstractEnvironment; rng::AbstractRNG, kwargs...)\n\nReset the environment to initial conditions based on the random number generator.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.environment_step!-Tuple{AbstractEnvironment,Any}","page":"Environments","title":"JuliaRL.environment_step!","text":"environment_step!(env::AbstractEnvironment, action; rng::AbstractRNG, kwargs...)\n\nUpdate the state of the environment based on the underlying dynamics and the action. This is not used directly, but through the step function.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.get_reward-Tuple{AbstractEnvironment}","page":"Environments","title":"JuliaRL.get_reward","text":"get_reward(env::AbstractEnvironment)\n\nRetrieve reward for the current state of the environment.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.is_terminal-Tuple{AbstractEnvironment}","page":"Environments","title":"JuliaRL.is_terminal","text":"is_terminal(env::AbstractEnvironment)\n\nCheck if the environment is in a terminal state\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.get_state-Tuple{AbstractEnvironment}","page":"Environments","title":"JuliaRL.get_state","text":"get_state(env::AbstractEnvironment)\n\nRetrieve the current state of the environment\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#JuliaRL.get_actions-Tuple{AbstractEnvironment}","page":"Environments","title":"JuliaRL.get_actions","text":"get_actions(env::AbstractEnvironment)\n\nReturns the set of actions available to take.\n\n\n\n\n\n","category":"method"},{"location":"docs/environments.html#","page":"Environments","title":"Environments","text":"To dig into the dynamics of the environment while running, we can also implement two interfaces for visualization.","category":"page"},{"location":"docs/environments.html#","page":"Environments","title":"Environments","text":"JuliaRL.render(env::AbstractEnvironment, args...; kwargs...)","category":"page"},{"location":"docs/environments.html#JuliaRL.render-Tuple{AbstractEnvironment,Vararg{Any,N} where N}","page":"Environments","title":"JuliaRL.render","text":"render(env::AbstractEnvironment, args...; kwargs...)\n\nRender the environment. (WIP, only works with Gym currently.)\n\n\n\n\n\n","category":"method"},{"location":"docs/agents.html#Agents-1","page":"Agents","title":"Agents","text":"","category":"section"},{"location":"docs/agents.html#","page":"Agents","title":"Agents","text":"TBD","category":"page"},{"location":"docs/learning.html#Learning-1","page":"Learning","title":"Learning","text":"","category":"section"},{"location":"docs/learning.html#Abstract-API-1","page":"Learning","title":"Abstract API","text":"","category":"section"},{"location":"docs/learning.html#","page":"Learning","title":"Learning","text":"Learning.AbstractValueFunction\nLearning.AbstractVFunction\nLearning.AbstractQFunction","category":"page"},{"location":"docs/learning.html#JuliaRL.Learning.AbstractValueFunction","page":"Learning","title":"JuliaRL.Learning.AbstractValueFunction","text":"AbstractValueFunctionn\n\nAbstract type definition for a value function\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#JuliaRL.Learning.AbstractVFunction","page":"Learning","title":"JuliaRL.Learning.AbstractVFunction","text":"AbstractVFunction\n\nAbstract type definition for a state value function\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#JuliaRL.Learning.AbstractQFunction","page":"Learning","title":"JuliaRL.Learning.AbstractQFunction","text":"AbstractQFunction\n\nAbstract type definition for a state-action value function\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#","page":"Learning","title":"Learning","text":"Learning.update!","category":"page"},{"location":"docs/learning.html#JuliaRL.Learning.update!","page":"Learning","title":"JuliaRL.Learning.update!","text":"update!(value::ValueFunction, opt::Optimizer, ρ, s_t, s_tp1, reward, γ, terminal)\n\nArguments:\n\nvalue::ValueFunction: opt::Optimizer: ρ: Importance sampling ratios (Array of Floats) s_t: States at time t s_tp1: States at time t + 1 reward: cumulant or reward for value function γ: discount factor terminal: Determining termination of the episode (if applicable).\n\n\n\n\n\nupdate!(value::ValueFunction, opt::Optimizer, ρ, s_t, s_tp1, reward, γ, terminal)\n\nArguments:\n\nvalue::ValueFunction: opt::Optimizer: ρ: Importance sampling ratios (Array of Floats) s_t: States at time t s_tp1: States at time t + 1 reward: cumulant or reward for value function γ: discount factor terminal: Determining termination of the episode (if applicable). a_t: Action at time t a_tp1: Action at time t + 1 target_policy: Action at time t\n\n\n\n\n\n","category":"function"},{"location":"docs/learning.html#LinearRL-1","page":"Learning","title":"LinearRL","text":"","category":"section"},{"location":"docs/learning.html#","page":"Learning","title":"Learning","text":"Learning.LinearRL.VFunction\nLearning.LinearRL.SparseVFunction\nLearning.LinearRL.QFunction\nLearning.LinearRL.SparseQFunction","category":"page"},{"location":"docs/learning.html#JuliaRL.Learning.LinearRL.VFunction","page":"Learning","title":"JuliaRL.Learning.LinearRL.VFunction","text":"VFunction(num_features)\n\nA structure hosting the weights for a linear value function. Used for Linear function approximation.\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#JuliaRL.Learning.LinearRL.SparseVFunction","page":"Learning","title":"JuliaRL.Learning.LinearRL.SparseVFunction","text":"SparseVFunction(num_features)\n\nA structure for when the feature vector is known to be sparse, and handed to the agent as a list of indices. Significantly faster than a normal value function in this special case.\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#JuliaRL.Learning.LinearRL.QFunction","page":"Learning","title":"JuliaRL.Learning.LinearRL.QFunction","text":"QFunction(num_features, num_features_per_action, num_actions)\n\nLinear QFunction. Assumes no sparsity, and an array of floats for a feature vector.\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#JuliaRL.Learning.LinearRL.SparseQFunction","page":"Learning","title":"JuliaRL.Learning.LinearRL.SparseQFunction","text":"SparseQFunction(num_features, num_features_per_action, num_actions)\n\nQFunction assuming sparsity.\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#","page":"Learning","title":"Learning","text":"Currently implemented algorithms","category":"page"},{"location":"docs/learning.html#","page":"Learning","title":"Learning","text":"Learning.LinearRL.TD\nLearning.LinearRL.TDLambda\nLearning.LinearRL.WatkinsQ","category":"page"},{"location":"docs/learning.html#JuliaRL.Learning.LinearRL.TD","page":"Learning","title":"JuliaRL.Learning.LinearRL.TD","text":"TD(α)\n\nOnline Temporal Difference Learning.\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#JuliaRL.Learning.LinearRL.TDLambda","page":"Learning","title":"JuliaRL.Learning.LinearRL.TDLambda","text":"TDLambda(α, λ)\n\nOnline Temporal Difference Learning with eligibility traces.\n\n\n\n\n\n","category":"type"},{"location":"docs/learning.html#JuliaRL.Learning.LinearRL.WatkinsQ","page":"Learning","title":"JuliaRL.Learning.LinearRL.WatkinsQ","text":"WatkinsQ(α)\n\nQ Learning as defined by Watkins\n\n\n\n\n\n","category":"type"},{"location":"docs/feature_creators.html#Feature-Creators-1","page":"Feature Creators","title":"Feature Creators","text":"","category":"section"},{"location":"docs/feature_creators.html#Abstract-API-1","page":"Feature Creators","title":"Abstract API","text":"","category":"section"},{"location":"docs/feature_creators.html#","page":"Feature Creators","title":"Feature Creators","text":"JuliaRL.FeatureCreators.AbstractFeatureCreator\nJuliaRL.FeatureCreators.create_features\nJuliaRL.FeatureCreators.feature_size","category":"page"},{"location":"docs/feature_creators.html#TileCoder-1","page":"Feature Creators","title":"TileCoder","text":"","category":"section"},{"location":"docs/feature_creators.html#","page":"Feature Creators","title":"Feature Creators","text":"FeatureCreators.TileCoder","category":"page"},{"location":"docs/feature_creators.html#JuliaRL.FeatureCreators.TileCoder","page":"Feature Creators","title":"JuliaRL.FeatureCreators.TileCoder","text":"TileCoder(num_tilings, num_tiles, num_features, num_ints)\n\nTile coder for coding all features together.\n\n\n\n\n\n","category":"type"}]
}
